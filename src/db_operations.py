import pandas as pd
import psycopg2 # For PostgreSQL
from psycopg2 import Error
import os

# --- Configuration for PostgreSQL ---

db_params = {
    "host": "localhost",
    "database": "bank_reviews", 
    "user": "postgres",          
    "password": "Pass@12345",      
    "port": "5433"                    
}

# --- Load Data ---
csv_file_path = "data/processed/reviews_with_sentiment.csv" # Updated path
try:
    df = pd.read_csv(csv_file_path)
    print(f"✅ Loaded '{csv_file_path}' successfully.")
except FileNotFoundError:
    print(f"❌ Error: '{csv_file_path}' not found. Please ensure the file is in the correct directory.")
    exit()
except Exception as e:
    print(f"❌ Error loading CSV: {e}")
    exit()

# --- Map DataFrame dtypes to PostgreSQL data types ---
def get_postgres_type(pandas_dtype, column_name):
    """
    Maps pandas data types to PostgreSQL data types.
    Handles specific column names for better type inference.
    """
    if pd.api.types.is_integer_dtype(pandas_dtype):
        return "INTEGER"
    elif pd.api.types.is_float_dtype(pandas_dtype):
        return "REAL"
    elif pd.api.types.is_datetime64_any_dtype(pandas_dtype) or column_name == 'date':
        # Assuming 'date' column in CSV is 'YYYY-MM-DD' string format, map to DATE
        return "DATE"
    elif pd.api.types.is_bool_dtype(pandas_dtype):
        return "BOOLEAN"
    else: # Default for object (strings)
        # Apply specific lengths for known string columns, otherwise TEXT
        if column_name in ['bank', 'source']:
            return "VARCHAR(50)"
        elif column_name == 'sentiment_label': # Sentiment label is usually short
            return "VARCHAR(10)"
        elif column_name == 'review': # Review content can be long
            return "TEXT"
        else:
            return "TEXT" # Default for any other string columns

connection = None
cursor = None
sql_dump_file = None

try:
    # --- Establish PostgreSQL Connection ---
    connection = psycopg2.connect(**db_params)
    cursor = connection.cursor()

    print("✅ PostgreSQL database connection successful.")

    table_name = "reviews"

    # --- Dynamically Create Table Schema ---
    columns_sql = []
    insert_cols = [] # Columns for INSERT statement (database column names)
    insert_placeholders = [] # Placeholders for INSERT statement (e.g., %s)

    # Mapping from original DataFrame column names to desired database column names
    # This helps standardize names (e.g., 'review_id' -> 'id', 'sentiment_label' -> 'sentiment')
    column_name_map = {
        'review_id': 'id',
        'sentiment_label': 'sentiment',
        'date': 'review_date'
    }

    for original_col, dtype in df.dtypes.items():
        db_col_name = column_name_map.get(original_col, original_col) # Get mapped name or original
        pg_type = get_postgres_type(dtype, original_col) # Get PG type based on original col name and dtype

        column_definition = f"{db_col_name} {pg_type}"
        if db_col_name == 'id': # Special handling for primary key
            # For IDENTITY columns, we just define type and PRIMARY KEY.
            # We don't add GENERATED BY DEFAULT AS IDENTITY here as it's part of the table creation,
            # not for every column definition, and we are dynamically building based on CSV content.
            # If 'id' is from CSV, it's explicitly inserted, so the GENERATED BY DEFAULT is overridden.
            column_definition += " PRIMARY KEY"

        columns_sql.append(column_definition)
        insert_cols.append(db_col_name) # Add to list for INSERT statement
        insert_placeholders.append("%s") # Add placeholder for INSERT statement

    create_table_sql = f"""
    CREATE TABLE IF NOT EXISTS {table_name} (
        {", ".join(columns_sql)}
    )
    """
    cursor.execute(create_table_sql)
    print(f"✅ Table '{table_name}' checked/created successfully based on CSV schema.")

    # --- Prepare SQL Dump File ---
    dump_folder = "data/postgress_data"
    os.makedirs(dump_folder, exist_ok=True)
    dump_filename = os.path.join(dump_folder, f"{table_name}.sql")
    sql_dump_file = open(dump_filename, 'w', encoding='utf-8')
    print(f"✅ Preparing SQL dump file: {dump_filename}")

    # Add CREATE TABLE statement to dump file for completeness
    sql_dump_file.write(f"""
-- SQL Dump for table: {table_name}
-- Generated by Python script

{create_table_sql};

-- Data Insertion
""")

    # --- Insert Data and Generate SQL Dump ---
    # Construct the base INSERT statement for the database
    insert_base_sql = f"""
INSERT INTO {table_name} ({", ".join(insert_cols)})
VALUES ({", ".join(insert_placeholders)})
ON CONFLICT (id) DO NOTHING; -- Handle duplicate primary keys gracefully
"""

    for index, row in df.iterrows():
        # Prepare values for actual database insert (psycopg2 handles None for NULL)
        insert_data_for_db = []
        # Prepare values for SQL dump string (manual sanitation for single quotes, NULLs)
        insert_data_for_dump = []

        # Iterate through the columns in the order they will be inserted
        for original_col in df.columns:
            # db_col_name is not directly used here, but its mapping is important for insert_cols
            val = row[original_col]
            pg_type_for_col = get_postgres_type(df.dtypes[original_col], original_col)

            # --- For psycopg2 database insert ---
            if pd.isna(val):
                insert_data_for_db.append(None)
            elif original_col == 'date':
                # Convert date strings to Python date objects for psycopg2
                # Handle cases where date might be non-date string (e.g., '2023-01-01') or already datetime
                if pd.api.types.is_string_dtype(df.dtypes[original_col]):
                    try:
                        insert_data_for_db.append(pd.to_datetime(val).date())
                    except ValueError:
                        insert_data_for_db.append(None) # Or handle invalid date strings as needed
                else:
                    insert_data_for_db.append(val)
            else:
                insert_data_for_db.append(val)

            # --- For SQL dump file string ---
            if pd.isna(val):
                insert_data_for_dump.append('NULL')
            elif pg_type_for_col in ["VARCHAR(50)", "TEXT", "VARCHAR(10)"]:
                # Sanitize string for SQL dump (escape single quotes by doubling them)
                dump_val = str(val).replace("'", "''")
                insert_data_for_dump.append(f"'{dump_val}'")
            elif pg_type_for_col == "DATE":
                # Ensure date is in 'YYYY-MM-DD' format and quoted for SQL dump
                dump_val = str(pd.to_datetime(val).date()) if pd.notna(val) else 'NULL' # Convert to YYYY-MM-DD string
                insert_data_for_dump.append(f"'{dump_val}'" if dump_val != 'NULL' else 'NULL')
            elif pg_type_for_col in ["INTEGER", "REAL", "BOOLEAN"]:
                insert_data_for_dump.append(str(val))
            else:
                # Fallback for other types, default to string representation, correctly escaped
                dump_val = str(val).replace("'", "''")
                insert_data_for_dump.append(f"'{dump_val}'")


        # Construct the SQL INSERT statement for the dump file (without ON CONFLICT for simplicity)
        sql_insert_statement = f"""
INSERT INTO {table_name} ({", ".join(insert_cols)})
VALUES ({", ".join(insert_data_for_dump)});
"""
        sql_dump_file.write(sql_insert_statement)

        # Execute the actual insert into the database
        cursor.execute(insert_base_sql, tuple(insert_data_for_db))

    # --- Commit Changes ---
    connection.commit()
    print(f"✅ Successfully inserted {len(df)} rows into '{table_name}' table.")

except (Exception, Error) as error:
    print(f"❌ Error while connecting to PostgreSQL or performing database operations: {error}")
    # Rollback in case of any error during the transaction
    if connection:
        connection.rollback()
        print("⚠️ Database transaction rolled back due to error.")

finally:
    # --- Close Connection and Dump File ---
    if cursor:
        cursor.close()
        print("✅ PostgreSQL cursor is closed.")
    if connection:
        connection.close()
        print("✅ PostgreSQL connection is closed.")
    if sql_dump_file:
        sql_dump_file.close()
        print(f"✅ SQL dump file '{dump_filename}' closed.")
